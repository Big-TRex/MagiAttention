{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
       "        [False,  True, False, False, False, False, False, False, False, False],\n",
       "        [False, False,  True, False, False, False, False, False, False, False],\n",
       "        [False, False, False,  True, False, False, False, False, False, False],\n",
       "        [False, False, False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False, False, False],\n",
       "        [False, False, False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True, False, False]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.logical_and(torch.ones(8, 10).triu_(), torch.ones(8, 10).tril_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(8, 10).triu_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(8, 10).tril_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_ranges(q_ranges, k_ranges, attn_type_map, q_len, k_len):\n",
    "    bsz = q_ranges.shape[0]\n",
    "    mask = torch.zeros((q_len, k_len), device='cuda', dtype=torch.bool)\n",
    "    for i in range(bsz):\n",
    "        if attn_type_map[i] == 1:\n",
    "            mask_slice = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]]\n",
    "            short_len = min(mask_slice.shape[0], mask_slice.shape[1])\n",
    "            causal_part = torch.ones(short_len, short_len, device=mask_slice.device, dtype=mask_slice.dtype).tril_()\n",
    "            mask_slice[-short_len:, -short_len:] = causal_part\n",
    "            mask_slice[:, :-short_len] = True\n",
    "            mask_slice[:-short_len, :] = False\n",
    "        elif attn_type_map[i] == 0:\n",
    "            mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]] = True\n",
    "        elif attn_type_map[i] == 2:\n",
    "            mask_slice = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]]\n",
    "            short_len = min(mask_slice.shape[0], mask_slice.shape[1])\n",
    "            inv_causal_part = torch.ones(short_len, short_len, device=mask_slice.device, dtype=mask_slice.dtype).triu_()\n",
    "            mask_slice[:short_len, :short_len] = inv_causal_part\n",
    "            mask_slice[:, short_len:] = True\n",
    "            mask_slice[short_len:, :] = False\n",
    "        else:\n",
    "            mask_slice_causal = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]].clone()\n",
    "            short_len = min(mask_slice_causal.shape[0], mask_slice_causal.shape[1])\n",
    "            causal_part = torch.ones(short_len, short_len, device=mask_slice_causal.device, dtype=mask_slice_causal.dtype).tril_()\n",
    "            mask_slice_causal[-short_len:, -short_len:] = causal_part\n",
    "            mask_slice_causal[:, :-short_len] = True\n",
    "            mask_slice_causal[:-short_len, :] = False\n",
    "            \n",
    "            mask_slice_inv_causal = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]].clone()\n",
    "            short_len = min(mask_slice_inv_causal.shape[0], mask_slice_inv_causal.shape[1])\n",
    "            inv_causal_part = torch.ones(short_len, short_len, device=mask_slice_inv_causal.device, dtype=mask_slice_inv_causal.dtype).triu_()\n",
    "            mask_slice_inv_causal[:short_len, :short_len] = inv_causal_part\n",
    "            mask_slice_inv_causal[:, short_len:] = True\n",
    "            mask_slice_inv_causal[short_len:, :] = False\n",
    "\n",
    "            mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]] = torch.logical_and(mask_slice_causal, mask_slice_inv_causal)\n",
    "\n",
    "                \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "        [False,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask_from_ranges(torch.tensor([[0, 4]], device='cuda'), torch.tensor([[0, 10]], device='cuda'), torch.tensor([3], device='cuda'), 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 48, 128]) torch.Size([4096, 48, 128]) torch.Size([4096, 48, 128])\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "from torch.nn.attention.flex_attention import create_block_mask\n",
    "from torch.nn.attention.flex_attention import flex_attention\n",
    "flex_attention_compiled = torch.compile(flex_attention)\n",
    "\n",
    "from flash_attn_interface import flex_flash_attn_func\n",
    "\n",
    "def get_mask_from_ranges(q_ranges, k_ranges, attn_type_map, q_len, k_len):\n",
    "    bsz = q_ranges.shape[0]\n",
    "    mask = torch.zeros((q_len, k_len), device='cuda', dtype=torch.bool)\n",
    "    for i in range(bsz):\n",
    "        if attn_type_map[i] == 1:\n",
    "            mask_slice = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]]\n",
    "            short_len = min(mask_slice.shape[0], mask_slice.shape[1])\n",
    "            causal_part = torch.ones(short_len, short_len, device=mask_slice.device, dtype=mask_slice.dtype).tril_()\n",
    "            mask_slice[-short_len:, -short_len:] = causal_part\n",
    "            mask_slice[:, :-short_len] = True\n",
    "            mask_slice[:-short_len, :] = False\n",
    "        elif attn_type_map[i] == 0:\n",
    "            mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]] = True\n",
    "        elif attn_type_map[i] == 2:\n",
    "            mask_slice = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]]\n",
    "            short_len = min(mask_slice.shape[0], mask_slice.shape[1])\n",
    "            inv_causal_part = torch.ones(short_len, short_len, device=mask_slice.device, dtype=mask_slice.dtype).triu_()\n",
    "            mask_slice[:short_len, :short_len] = inv_causal_part\n",
    "            mask_slice[:, short_len:] = True\n",
    "            mask_slice[short_len:, :] = False\n",
    "        else:\n",
    "            mask_slice_causal = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]].clone()\n",
    "            short_len = min(mask_slice_causal.shape[0], mask_slice_causal.shape[1])\n",
    "            causal_part = torch.ones(short_len, short_len, device=mask_slice_causal.device, dtype=mask_slice_causal.dtype).tril_()\n",
    "            mask_slice_causal[-short_len:, -short_len:] = causal_part\n",
    "            mask_slice_causal[:, :-short_len] = True\n",
    "            mask_slice_causal[:-short_len, :] = False\n",
    "            \n",
    "            mask_slice_inv_causal = mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]].clone()\n",
    "            short_len = min(mask_slice_inv_causal.shape[0], mask_slice_inv_causal.shape[1])\n",
    "            inv_causal_part = torch.ones(short_len, short_len, device=mask_slice_inv_causal.device, dtype=mask_slice_inv_causal.dtype).triu_()\n",
    "            mask_slice_inv_causal[:short_len, :short_len] = inv_causal_part\n",
    "            mask_slice_inv_causal[:, short_len:] = True\n",
    "            mask_slice_inv_causal[short_len:, :] = False\n",
    "\n",
    "            mask[q_ranges[i, 0]:q_ranges[i, 1], k_ranges[i, 0]:k_ranges[i, 1]] = torch.logical_and(mask_slice_causal, mask_slice_inv_causal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_with_cuda_event(name, flops):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    torch.cuda.nvtx.range_push(name)\n",
    "    start.record()\n",
    "    yield\n",
    "    end.record()\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    end.synchronize()\n",
    "\n",
    "    elapsed_time = start.elapsed_time(end)\n",
    "    mfu = flops / (elapsed_time * 0.989 * 1e12)\n",
    "    print(f\"{name} took {elapsed_time} ms, mfu: {mfu:.2f}\")\n",
    "\n",
    "query = torch.randn(1, 48, 4096, 128, device='cuda', dtype=torch.bfloat16)\n",
    "key = torch.randn(1, 48, 4096, 128, device='cuda', dtype=torch.bfloat16)\n",
    "value = torch.randn(1, 48, 4096, 128, device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "query_thd = query.squeeze().transpose(0, 1)\n",
    "key_thd = key.squeeze().transpose(0, 1)\n",
    "value_thd = value.squeeze().transpose(0, 1)\n",
    "print(query_thd.shape, key_thd.shape, value_thd.shape)\n",
    "\n",
    "query_thd.shape, key_thd.shape, value_thd.shape\n",
    "q_ranges = torch.tensor([[0, 1024], [1024, 4096]], device='cuda', dtype=torch.int32)\n",
    "k_ranges = torch.tensor([[0, 1024], [0, 4096]], device='cuda', dtype=torch.int32)\n",
    "attn_type_map = torch.tensor([1, 3], device='cuda', dtype=torch.int32)\n",
    "\n",
    "mask = get_mask_from_ranges(q_ranges, k_ranges, attn_type_map, 4096, 4096)\n",
    "\n",
    "warmup_iters = 10\n",
    "test_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLIDING_WINDOW = 1024\n",
    "\n",
    "def sliding_window_causal(b, h, q_idx, kv_idx):\n",
    "    causal_mask = q_idx >= kv_idx\n",
    "    window_mask = q_idx - kv_idx <= SLIDING_WINDOW \n",
    "    return causal_mask & window_mask\n",
    "\n",
    "# Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them) \n",
    "block_mask = create_block_mask(sliding_window_causal, B=None, H=None, Q_LEN=4096, KV_LEN=4096)\n",
    "# In this case, we don't need a score_mod, so we won't pass any in.\n",
    "# However, score_mod can still be combined with block_mask if you need the additional flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flex_attention took 35.26921463012695 ms, mfu: 0.26\n",
      "flex_flash_attn took 22.082239151000977 ms, mfu: 0.41\n"
     ]
    }
   ],
   "source": [
    "# swa\n",
    "flops = 6144 * 4 * (1024 * 1024 / 2 + 1024 * 3072)\n",
    "total_flops = flops * test_iters\n",
    "for i in range(warmup_iters):\n",
    "    o = flex_attention_compiled(query, key, value, block_mask=block_mask)\n",
    "\n",
    "with time_with_cuda_event(\"flex_attention\", total_flops):\n",
    "    for i in range(test_iters):\n",
    "        o = flex_attention_compiled(query, key, value, block_mask=block_mask)\n",
    "\n",
    "for i in range(warmup_iters):\n",
    "    o_thd, _ = flex_flash_attn_func(query_thd, key_thd, value_thd, q_ranges, k_ranges, max_seqlen_q=3072, max_seqlen_k=4096, attn_type_map=attn_type_map)\n",
    "\n",
    "with time_with_cuda_event(\"flex_flash_attn\", total_flops):\n",
    "    for i in range(test_iters):\n",
    "        o_thd, _ = flex_flash_attn_func(query_thd, key_thd, value_thd, q_ranges, k_ranges, max_seqlen_q=3072, max_seqlen_k=4096, attn_type_map=attn_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "# Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them) \n",
    "block_mask = create_block_mask(causal, B=None, H=None, Q_LEN=4096, KV_LEN=4096)\n",
    "q_ranges = torch.tensor([[0, 4096]], device='cuda', dtype=torch.int32)\n",
    "k_ranges = torch.tensor([[0, 4096]], device='cuda', dtype=torch.int32)\n",
    "attn_type_map = torch.tensor([1], device='cuda', dtype=torch.int32)\n",
    "\n",
    "mask = get_mask_from_ranges(q_ranges, k_ranges, attn_type_map, 4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flex_attention took 69.78524780273438 ms, mfu: 0.30\n",
      "flex_flash_attn took 37.469600677490234 ms, mfu: 0.56\n"
     ]
    }
   ],
   "source": [
    "# causal\n",
    "flops = 6144 * 4 * (4096 * 4096 / 2)\n",
    "total_flops = flops * test_iters\n",
    "for i in range(warmup_iters):\n",
    "    o = flex_attention_compiled(query, key, value, block_mask=block_mask)\n",
    "\n",
    "with time_with_cuda_event(\"flex_attention\", total_flops):\n",
    "    for i in range(test_iters):\n",
    "        o = flex_attention_compiled(query, key, value, block_mask=block_mask)\n",
    "\n",
    "for i in range(warmup_iters):\n",
    "    o_thd, _ = flex_flash_attn_func(query_thd, key_thd, value_thd, q_ranges, k_ranges, max_seqlen_q=3072, max_seqlen_k=4096, attn_type_map=attn_type_map)\n",
    "\n",
    "with time_with_cuda_event(\"flex_flash_attn\", total_flops):\n",
    "    for i in range(test_iters):\n",
    "        o_thd, _ = flex_flash_attn_func(query_thd, key_thd, value_thd, q_ranges, k_ranges, max_seqlen_q=3072, max_seqlen_k=4096, attn_type_map=attn_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ranges = torch.tensor([[0, 4096]], device='cuda', dtype=torch.int32)\n",
    "k_ranges = torch.tensor([[0, 4096]], device='cuda', dtype=torch.int32)\n",
    "attn_type_map = torch.tensor([0], device='cuda', dtype=torch.int32)\n",
    "\n",
    "mask = get_mask_from_ranges(q_ranges, k_ranges, attn_type_map, 4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flex_attention took 110.56114959716797 ms, mfu: 0.38\n",
      "flex_flash_attn took 65.88972473144531 ms, mfu: 0.63\n"
     ]
    }
   ],
   "source": [
    "# full\n",
    "flops = 6144 * 4 * (4096 * 4096)\n",
    "total_flops = flops * test_iters\n",
    "for i in range(warmup_iters):\n",
    "    o = flex_attention_compiled(query, key, value)\n",
    "\n",
    "with time_with_cuda_event(\"flex_attention\", total_flops):\n",
    "    for i in range(test_iters):\n",
    "        o = flex_attention_compiled(query, key, value)\n",
    "\n",
    "for i in range(warmup_iters):\n",
    "    o_thd, _ = flex_flash_attn_func(query_thd, key_thd, value_thd, q_ranges, k_ranges, max_seqlen_q=3072, max_seqlen_k=4096, attn_type_map=attn_type_map)\n",
    "\n",
    "with time_with_cuda_event(\"flex_flash_attn\", total_flops):\n",
    "    for i in range(test_iters):\n",
    "        o_thd, _ = flex_flash_attn_func(query_thd, key_thd, value_thd, q_ranges, k_ranges, max_seqlen_q=3072, max_seqlen_k=4096, attn_type_map=attn_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
