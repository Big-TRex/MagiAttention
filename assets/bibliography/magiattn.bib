@article{jacobs2023deepspeed,
  title={Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv preprint arXiv:2309.14509},
  year={2023},
  url={https://arxiv.org/pdf/2309.14509}
}

@article{liu2023ringattentionblockwisetransformers,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}


@misc{fang2024uspunifiedsequenceparallelism,
      title={USP: A Unified Sequence Parallelism Approach for Long Context Generative AI}, 
      author={Jiarui Fang and Shangchun Zhao},
      year={2024},
      eprint={2405.07719},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07719}, 
}

@misc{chen2024longvilascalinglongcontextvisual,
      title={LongVILA: Scaling Long-Context Visual Language Models for Long Videos}, 
      author={Yukang Chen and Fuzhao Xue and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Ethan He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han},
      year={2024},
      eprint={2408.10188},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.10188}, 
}

@misc{gu2024loongtrainefficienttraininglongsequence,
      title={LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism}, 
      author={Diandian Gu and Peng Sun and Qinghao Hu and Ting Huang and Xun Chen and Yingtong Xiong and Guoteng Wang and Qiaoling Chen and Shangchun Zhao and Jiarui Fang and Yonggang Wen and Tianwei Zhang and Xin Jin and Xuanzhe Liu},
      year={2024},
      eprint={2406.18485},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.18485}, 
}

@misc{wang2024datacentricheterogeneityadaptivesequenceparallelism,
      title={Data-Centric and Heterogeneity-Adaptive Sequence Parallelism for Efficient LLM Training}, 
      author={Yujie Wang and Shiju Wang and Shenhan Zhu and Fangcheng Fu and Xinyi Liu and Xuefeng Xiao and Huixia Li and Jiashi Li and Faming Wu and Bin Cui},
      year={2024},
      eprint={2412.01523},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2412.01523}, 
}

@misc{zhang2024dcp,
    title  = {Training Variable Sequences with Data-Centric Parallel},
    author = {Geng Zhang and Xuanlei Zhao and Kai Wang and Yang You},
    year   = {2024},
}

@misc{ge2025bytescaleefficientscalingllm,
    title         = {ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs},
    author        = {Hao Ge and Junda Feng and Qi Huang and Fangcheng Fu and Xiaonan Nie and Lei Zuo and Haibin Lin and Bin Cui and Xin Liu},
    year          = {2025},
    eprint        = {2502.21231},
    archiveprefix = {arXiv},
    primaryclass  = {cs.DC},
    url           = {https://arxiv.org/abs/2502.21231},
}
