## Intergrate magiattention with megatron
We fork a repo of megatron and show how to intergrate MagiAttention with Megatron.

You can refer to  https://github.com/hanwen-sun/Megatron-LM/pull/2 for more information.

## High-Precision Alignment
We train llama-1b model from scratch with MagiAttention and compare the training loss convergence curve with te context parallel:
![alt text](./results.png)
You can refer to https://github.com/hanwen-sun/Megatron-LM/pull/2 for more infromation.
